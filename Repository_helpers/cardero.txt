Doing:
Continue adding base for integration testing

Integration testing:
  - clear error message when fail to initially load spec
  - it makes sense to use Gulp-Jasmine only if it can be run in watch mode and
    trigger only tests of files being changed
     - otherwise should use execCommand()
  - parallelism:
     - one it() per TEST_NAME
        - it("METHOD /PATH should respond with STATUS_CODE (TEST_NAME)")
           - METHOD and /PATH should be padded left
        - do not use any describe()
     - inside each it() run several HTTP requests:
        - all at same time (Promise.all()-like)
        - can have several RESP.x-test-params
        - each RESP.x-test-param itself generates several runs:
        - tests here means not only each it() but any HTTP request
     - if any HTTP request fails, whole it() fails:
        - should only show first failure
        - other HTTP requests should be canceled
           - if cannot, try to at least unref() them
        - error message should show both the specific RESP.x-test-param and the generated value
     - should run all it() in parallel
        - in different microloops (for async parallelism)
        - in different processes (for CPU parallelism)
        - maybe switch to ava to get that
     - should limit number of concurrent HTTP requests while still maximize it
        - try to guess the value???
  - data-driven test:
     - each TEST_NAME should generate OPTS.repeat NUM (def: 10) actual tests
     - all actual tests are run in parallel
     - each actual test generates its own actual parameters using JSON-SCHEMA-FAKER on PARAM SCHEMA
        - OPTS.optionalsProbability 0.3
        - add OpenAPI "format"s
        - arrays should never have duplicate ids???
        - stop populating new array elements when cannot anymore because uniqueItems or no more ids???
     - add fuzzy testing values???
  - tests:
     - { TEST_NAME: REQUEST }
     - can either be???
        - in a separate file
        - as RESP.x-test
           - in which case do not need to specify status nor path nor method
     - endpoint + response combinations with no tests do not produce any tests
        - as opposed to defaulting to trying the 2** response
     - gulp openapi should validate syntax
        - including avoiding infinite recursions when requiring other tests with 'TEST_NAME.*'
  - REQUEST:
     - method 'METHOD'
     - path '/PATH'
     - status 'STATUS_CODE'
     - parameters.[IN_]PARAM_NAME SCHEMA|undefined|null
        - if starts with IN_ try that first
           - but if cannot find, consider IN_ to be part of PARAM_NAME
        - maybe prefixing with "parameters" is not needed since can add "IN_" to distinguish???
        - OpenAPI SCHEMA not JSON schema, i.e. must be converted to one
        - undefined|null means PARAM is not set
        - unless undefined|null, deep merged (with priority) to PARAM's SCHEMA
        - any schema or subschema true|false is converted to PARAM's SCHEMA and { not: PARAM's SCHEMA }
        - default value:
           - true (i.e. PARAM SCHEMA) if required PARAM
              - should path PARAM default to previous test (when using 'TEST_NAME.parameters.*') if it has the same PARAM_NAME???
           - undefined otherwise
        - should use PATHDEF|OPERATION.parameters, PARAM.name|in
     - response|headers SCHEMA:
        - like RESP.schema|headers but for those possible params
        - combined as union (not intersection nor override) of RESP.schema|headers
  - chaining requests:
     - any string in any SCHEMA in any test can be 'TEST_NAME.parameters|response|headers[.VARR]'
     - it will be replaced by { const: VAL } where VAL is based on that other TEST_NAME
     - TEST_NAME need to be run before the test:
        - it is run once for each test that requires it, not once for all, i.e. tests are independent from each other
        - if it fails (or has previously failed for another test), mark the test as skipped, not failed
  - Security:
     - x-test-params.[TYPE_]SEC_NAME "ROLE"
     - "ROLE" must be translated into a token used as query|header value:
        - maybe auth endpoints can declare a specific response header or body property as containing that token???
            - then those calls are performed before integration testing (and can be considered part of testing themselves)
        - if cannot translate, "ROLE" is left as is
     - assign one auto-generated user per "ROLE"???
     - remember that operations securities are OBJ_ARR, i.e. alternatives of unions
     - what about steps that cannot be automated, e.g. email checking or 2 factor auth???
        - during signup
        - during signin
     - use SPEC|OPERATION.security, SPEC.securityDefinitions
  - HTTP request:
     - use CROSS-FETCH (not FRISBY)
        - maybe replace current dependency REQUEST by CROSS-FETCH too
     - fix host|port???
        - I had to comment it out because requiring 'src/config' creates problem with gulp unitTest
           - maybe require('src/config') should be inside gulp integrationTest function
        - current host|port might not work on other environment, e.g. stage???
     - Content-Type [C] and Accept [C] according to SPEC|OPERATION.consumes|produces
  - Validation:
     - RESP STATUS_CODE
     - RESP.[x-test-params.*.]schema
     - RESP.[x-test-params.*.]headers.HEADER:
        - key is present??? how to validate optional header's value???
        - value matches SCHEMA
     - Content-Type [S], Accept [S] against SPEC|OPERATION.produces
     - check SWAY's validateResponse()
     - should use ajv???
     - what to do with 'file' PARAMs???
        - OPENAPI-SCHEMA-TO-JSON-SCHEMA throws on them
     - should also validate PARAM.allowEmptyValue
        - it is removed by OPENAPI-SCHEMA-TO-JSON-SCHEMA
  - Parsing|serializing:
     - parsing RESP.headers
     - serializing PARAM
     - including collectionFormat
     - check SWAY's convertValue() and validateResponse()
     - check SWAGGER-JS
  - Global state:
     - either???
        - before all tests, connect and create new database.
          At end of all tests (including if failed or uncaught exception), delete that database and disconnect)
        - same but instead of using a temporary database, keep track on new model ids created???
          Might not work, but would be database-independent
     - servers and databases must be running???
  - test database populating:
     - reason: tests should not share state:
     - at beginning of integration testing:
        - use SPEC.definitions for the list of models???
        - use databaseAdapter.create(OBJ_ARR)???
           - probably use MongoDB native driver, not Mongoose
     - each HTTP request should get its own set of models:
        - i.e. models related to each other but not to other sets of models
        - should be NUM models per model type, where NUM is hard-coded
  - relations:
     - format "[MODEL ]id[ TYPE]"
        - no MODEL if primary id of current model
           - specific MODEL constant means "not existing model"???
        - TYPE???
           - ObjectId, ID in URL, UUID, etc.
           - defaults to OPTS.idFormat
              - which defaults itself to databaseAdapter.idFormat
     - during initial fake data generation:
        - create all primary ids before creating the models
        - translate format "... id ..." into JSON schema property "enum" using those generated primary ids
           - only for models within same set of model (i.e. requests remain independent from each other)
        - pass list of primary ids attribute names to databaseAdapter.create()
     - during PARAM data generation:
        - generate using "enum" like for initial fake data generation
     - during response body|headers validation:
        - create custom format validator that checks against TYPE
     - populating relations:
        - what about nested/denormalized relations???
           - some might be able to also be created top-level, some not
        - what about relations that can either be an id or a populated relation???
           - depending on whether in input or in output
           - depending on other parameters, e.g. 'populate' variable
  - also test middleware (not endpoints)
  - Explore:
     - data-driven tests
     - fuzzy testing
     - stress testing
     - load testing
     - test coverage
     - test runners, reporting
  - future features:
     - add support for schemes:
        - use both SPEC.schemes and OPERATION.schemes
        - one test per scheme
        - ws[s]:
     - add support for OAuth2
     - JSON schema v7:
        - allow v7 in OpenAPI SCHEMA
        - problems:
           - not supported by OpenAPI yet
              - i.e. should be prefixed with x-*
              - should then converted by OPENAPI-SCHEMA-TO-JSON-SCHEMA (and inverse). Do a PR.
           - does not work with JSON-SCHEMA-FAKER yet

To do:
  - integration testing
  - Node 9
  - consider if can use OpenAPI 3.0
  - fix configuration, it's a little messy
     - implies learning about configuration tools first
  - add Flow
  - RSS/Atom
  - generate client SDKs from OpenAPI: JavaScript, Bash, others
  - update API documentation to Widdershins
  - lint-staged currently does not allow committing only half of a file
  - create fake|test database from OpenAPI:
     - can maybe specify in OpenAPI if a specific type of data is prefered
        - e.g. at least one user should be admin
        - can use faker, etc.
     - can replace populating scripts
     - can be merged with the test populating task at beginning of integration tests
  - use JSON schema instead of joi:
     - probably using SWAY.validateRequest|Response()
     - first use JSON schema during integration testing, then suggesting merging
        - maybe use enjoi as transition
     - possible advantages of JSON schemas:
        - declarative:
           - can be communicated to clients so they can validate before sending to APIs
              - there are even library to generate <form> and <input>
           - schema is data not code, i.e. easier to manipulate for other purpose
              - e.g. can be used for auto-documentation
        - we do not want to be loose on validation with sanitization, but fail hard instead
           - joi is built as sanitizer, e.g. every validation creates a deep copy
        - not JavaScript-specific
        - https://github.com/icebob/validator-benchmark
     - also validate response
     - also parse request parameters
  - replace server-driven populating by client-driven populating:
     - first refactor populating so that each route declaratively specify which models to populate instead of
       doing it imperatively
     - then let clients decide it with a query parameter instead of declaring it on the routes
     - same for:
       - filter
       - aggregation
       - pagination
       - sorting
       - population
  - generate Mongoose models from OpenAPI specification
  - add GraphQL:
     - swagger-to-graphql would allow automating it while still supporting REST
     - I estimated it at 2 days of work
  - other possible uses of OpenAPI:
     - server-side routing
     - web automation
     - mock server
  - yamllint

To do (front-end app):
  - keep .editorconfig in sync with back-end one

Ideas:
  - gamification:
     - simple overall/aggregate ranking number
     - widget/badges showing "certified by"
     - rankings, "best of the week"
     - think of potentialpark

Tools to update:
  - MongoDB, mongoose
  - nvm
  - cors
  - helmet
  - moment
  - morgan
  - winston, winston-mongodb, winston-sentry
  - lodash

Tools to learn (high priority):
  - Flow (they mentioned adding it):
     - maybe also TypeScript
     - go through to_learn to check related projects too
  - Heroku
  - Circle CI
  - ElasticSearch
  - documentation, static website generation (for API doc generation)
  - web security:
     - snyk

Tools to learn (priority):
  - yarn
  - Sentry
  - HTTPS
  - Mailgun
  - Twilio
  - Stripe

Tools to learn:
  - dotenv, nconf
  - express-rate-limit
  - express-useragent
  - geoip-lite
  - libphonenumber-js
  - husky, lint-staged
