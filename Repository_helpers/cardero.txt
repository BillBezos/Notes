Done
PRs review: Config database table, List API endpoints

Doing:
Continue adding base for integration testing

Integration testing:
  - merge feature/same_dependencies back to development
  - clear error message when fail to initially load spec
  - responsibility of the caller:
      - server should be running
      - mocking what might be hard to test or create undesirable side-effects, e.g.:
         - rate limiting
         - email|SMS sending|verification
         - logging
      - optionally a temporary database can be the target of the server
      - optional server exit and temporary database deletion
      - in my case:
         - create a config environment INTEGRATION
         - do the above when it is set
         - Gulp task should run then stop a server under that config environment
         - should then pass the config host|port to testing utility
            - i.e. they should be the same whether local or on stage
  - parallelism:
     - one it() per TEST_NAME
        - it("METHOD /PATH should respond with STATUS_CODE (TEST_NAME)")
           - METHOD and /PATH should be padded left
        - do not use any describe()
     - should run all it() in parallel
        - in different microloops (for async parallelism)
        - in different processes (for CPU parallelism) (not as important)
        - maybe switch to ava to get that
     - OPTS.maxParallel:
        - max number of concurrent HTTP requests (i.e. "actual tests")
        - default:
           - probably ulimit -n minus a constant
           - try on my own machine where it woukd start crashing
  - HTTP request:
     - use CROSS-FETCH (not FRISBY)
        - maybe replace current dependency REQUEST by CROSS-FETCH too
     - Content-Type [C] and Accept [C] according to SPEC|OPERATION.consumes|produces
     - increase default timeout from test runner
  - serializing|parsing:
     - serializing PARAM
     - parsing RESP.headers
     - including collectionFormat
     - check SWAY's convertValue() and validateResponse()
     - check SWAGGER-JS
  - validation:
     - RESP STATUS_CODE
     - RESP.schema, TEST.response
     - RESP|TEST.headers.HEADER:
        - key is present
        - value matches SCHEMA
     - Content-Type [S], Accept [S] against SPEC|OPERATION.produces
     - check SWAY's validateResponse()
     - should use ajv:
        - should compile schema, and memoize compilation using stable JSON stringify
     - what to do with 'file' PARAMs???
        - OPENAPI-SCHEMA-TO-JSON-SCHEMA throws on them
     - should also validate PARAM.allowEmptyValue
        - it is removed by OPENAPI-SCHEMA-TO-JSON-SCHEMA
  - data-driven test:
     - each TEST_NAME should generate OPTS.repeat NUM (def: 10) actual tests
     - all actual tests are run in parallel
     - all actual tests are independent from each other (e.g. their required TEST_NAMEs requests are separate)
     - if any actual test fails, whole TEST_NAME fails:
        - should only show first failure
        - other actual tests should be canceled
           - if cannot, try to at least unref() them
        - error message should show both the specific PARAM SCHEMA and the generated value
     - each actual test generates its own actual parameters using JSON-SCHEMA-FAKER on PARAM SCHEMA
        - OPTS.optionalsProbability 0.3
        - add OpenAPI "format"s
     - add fuzzy testing values???
        - maybe propose as an option to JSON-SCHEMA-FAKER???
  - RESP.x-tests.TEST_NAME REQUEST
     - RESP.x-tests defaults to {}, i.e. no tests
     - gulp openapi should validate syntax
        - no duplicate TEST_NAME for a given OPERATION
        - avoid infinite recursions when requiring other tests with 'OPERATIONID.TEST_NAME.*'
     - should be cleaned|removed from the OpenAPI specifications served either from the static
       server or from the API documentation
  - REQUEST:
     - [IN_]PARAM_NAME SCHEMA|undefined|null
        - if starts with IN_ try that first
           - but if cannot find, consider IN_ to be part of PARAM_NAME
        - OpenAPI SCHEMA not JSON schema, i.e. must be converted to one
        - undefined|null means PARAM is not set
        - unless undefined|null, deep merged (with priority) to PARAM's SCHEMA
        - any schema or subschema true|false is converted to PARAM's SCHEMA and { not: PARAM's SCHEMA }
        - default value:
           - true (i.e. PARAM SCHEMA) if required PARAM
           - undefined otherwise
        - should use PATHDEF|OPERATION.parameters, PARAM.name|in
        - how to specify "not existing model id"???
     - [TYPE_]SEC_NAME SCHEMA|undefined|null:
        - same as PARAM_NAME but using SPEC|OPERATION.security, SPEC.securityDefinitions
           - translated to query|header parameter
        - remember that operations securities are OBJ_ARR, i.e. alternatives of unions
     - response|headers SCHEMA:
        - like RESP.schema|headers but for those possible params
        - combined as union (not intersection nor override) of RESP.schema|headers
  - chaining requests:
     - any string in any SCHEMA in any test can be 'OPERATIONID.TEST_NAME.[IN_]PARAM_NAME|[TYPE_]SEC_NAME|response|headers[.VARR]'
     - it will be replaced by { const: VAL } where VAL is based on that other TEST_NAME
     - TEST_NAME will be run before the test:
        - it is run once for each test that requires it, not once for all, i.e. tests are independent from each other
        - if it fails (or has previously failed for another test), mark the test as skipped, not failed
     - caching:
        - should requests results of some tests be cached for speedup???
        - might be a bad idea as it would create shared state???
  - also test middleware (not endpoints)
  - Explore:
     - data-driven tests
     - test runners, reporting
     - fuzzy testing
     - stress testing
     - load testing
     - test coverage
  - future features:
     - To improve???
        - reduce amount of mocking needed by users
        - reduce amount of setup and teardown (e.g. starting server) needed by users
     - pass options to underlying test runner, e.g. reporters, reporting options, include|exclude, etc.
        - all those concerns should be offloaded to the test runner
     - add support for schemes:
        - use both SPEC.schemes and OPERATION.schemes
        - one test per scheme
        - ws[s]:
     - add support for OAuth2
     - JSON schema v7:
        - allow v7 in OpenAPI SCHEMA
        - problems:
           - not supported by OpenAPI yet
              - i.e. should be prefixed with x-*
              - should then converted by OPENAPI-SCHEMA-TO-JSON-SCHEMA (and inverse). Do a PR.
           - does not work with JSON-SCHEMA-FAKER yet

To do:
  - integration testing
  - clearly mark which libraries are the blockers for OpenAPI 3.0
  - Node 10
     - including Stream.pipeline(...)
  - Gulp:
     - gulp buildwatch does not work because of json-refs caching (https://github.com/whitlockjc/json-refs/pull/133)
     - CTRL-C does not work after gulp buildwatch and after one watch has been triggered
  - fix configuration, it's a little messy
     - implies learning about configuration tools first
  - add Flow
  - RSS/Atom
  - generate client SDKs from OpenAPI: JavaScript, Bash, others
  - update API documentation to Widdershins
  - lint-staged currently does not allow committing only half of a file
  - create fake|test database from OpenAPI:
     - can maybe specify in OpenAPI if a specific type of data is prefered
        - e.g. at least one user should be admin
        - can use faker, etc.
     - can replace populating scripts
     - can be merged with the test populating task at beginning of integration tests
  - use JSON schema instead of joi:
     - probably using SWAY.validateRequest|Response()
     - first use JSON schema during integration testing, then suggesting merging
        - maybe use enjoi as transition
     - possible advantages of JSON schemas:
        - declarative:
           - can be communicated to clients so they can validate before sending to APIs
              - there are even library to generate <form> and <input>
           - schema is data not code, i.e. easier to manipulate for other purpose
              - e.g. can be used for auto-documentation
        - we do not want to be loose on validation with sanitization, but fail hard instead
           - joi is built as sanitizer, e.g. every validation creates a deep copy
        - not JavaScript-specific
        - https://github.com/icebob/validator-benchmark
     - also validate response
     - also parse request parameters
  - replace server-driven populating by client-driven populating:
     - first refactor populating so that each route declaratively specify which models to populate instead of
       doing it imperatively
     - then let clients decide it with a query parameter instead of declaring it on the routes
     - same for:
       - filter
       - aggregation
       - pagination
       - sorting
       - population
  - generate Mongoose models from OpenAPI specification
  - add GraphQL:
     - swagger-to-graphql would allow automating it while still supporting REST
     - I estimated it at 2 days of work
  - other possible uses of OpenAPI:
     - server-side routing
     - web automation
     - mock server
  - yamllint

To do (front-end app):
  - keep .editorconfig in sync with back-end one

Ideas:
  - gamification:
     - simple overall/aggregate ranking number
     - widget/badges showing "certified by"
     - rankings, "best of the week"
     - think of potentialpark

Tools to update:
  - MongoDB, mongoose
  - nvm
  - cors
  - helmet
  - moment
  - morgan
  - winston, winston-mongodb, winston-sentry
  - lodash

Tools to learn (high priority):
  - Flow (they mentioned adding it):
     - maybe also TypeScript
     - go through to_learn to check related projects too
  - Heroku
  - Circle CI
  - ElasticSearch
  - documentation, static website generation (for API doc generation)
  - web security:
     - snyk

Tools to learn (priority):
  - yarn
  - Sentry
  - HTTPS
  - Mailgun
  - Twilio
  - Stripe

Tools to learn:
  - dotenv, nconf
  - express-rate-limit
  - express-useragent
  - geoip-lite
  - libphonenumber-js
  - husky, lint-staged
