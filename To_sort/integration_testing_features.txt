Check test runners features
  - fix Test running paragraph below

Allow test option to be a directory searched recursively

Reporting:
  - "tap" plugin:
     - produces TAP using test|error
        - use own TAP producing code, combining strengths of supertap and tape
     - do it on:
        - 'start' (TAP version, unique test)
        - 'complete' (assertion)
        - 'end' (plan, closing comments, closing the stream)
     - should build title there
     - should use YAML props for errors
        - including expected|actual
        - including full error.task
     - create a transformStream passed to config.tap.stream and use transformStream.write()
     - config.tap.ordered BOOL
        - def: false|true???
        - ordered TAP output
            - i.e. it follows tasks files list and object keys order
        - it does so by buffering output until its order comes
  - "report" plugin:
     - pipe config.tap.stream to a TAP reporter, then to stdout|file
     - CONF.report.output BOOL|STR:
        - true (def): stdout
        - false: silent
        - "PATH": file
     - CONF.report.style "REPORTER"
        - hand picked set of best TAP reporters for each category
        - are separate plugins:
           - "REPORTER" is module name, looked up in node modules
           - reporters have different API than other plugins
           - use dynamic require()
     - CONF.report.options OBJ:
        - passed to reporters
     - create our reporters:
        - 'tap': printing TAP as is
        - 'default' (def):
           - our own best TAP reporter specialized for our purposes
           - only prints failure but prints successful ones temporarily (progress bar)
           - should use error.task to:
              - show "Response was:..." if response body or header validation error (include emptiness check)
              - then show "Request was:...
           - show diff between error.expected and error.actual if both present
        - 'object':
           - JSON, YAML, etc., using autoformat
           - same data as what the return value would be
     - memory leaks:
        - if false pipe to dev null to free memory???
           - make some tests with small program receiving huge input and check memory usage
        - what if 'tap' plugin is used but not 'report' plugin???
  - allow tests GLOB to be "-" to refer to stdin???

CLI options:
  - get all possible plugins using readdir list from plugins dir
      - add TODO to use node_modules instead
  - then require all plugins
  - build CLI options descriptions using PLUGIN.conf.general|task.description|default|schema.type|required|others? and PLUGIN.conf.examples
  - think of type "object" and "array"
  - check code from autoserver
  - default value should be appended to description instead of using yargs.default()
  - task options should be added as an option under task category but throw error during parsing. An explanation at beginning of --help should explain how task options work

Enforce plugins separation by filter conf|task.* input and output to plugin.[optional]dependencies

Skip/only:
  - should be a plugin
  - skip has priority over only
  - TASK.skip|only BOOL
  - CONF.skip|only "GLOB"[_ARR]
     - same as setting TASK.skip|only true on test whose key matches GLOB
     - not applied on TASKs where TASK.skip|only is explicitely false
  - "only" filters tasks out in beginning of start handlers
     - for performance
     - i.e. not reported
  - "skip" throws at beginning of each task, setting ERROR.skipped true
     - TAP reporter mark this as # SKIP
     - i.e. task is still going through start handlers, e.g. validated
  - remove "dry" plugin, redundant with config.skip "*"

Should add headers added by default by node-fetch to returnValue.request.*???

Support collectionFormat `multi` on query parameter serialization

Fix README.md

Fix api-service

Fix this doc with new names (e.g. "task" instead of test)

Test running:
  - should run NUM it() in parallel
     - def maxParallel should be 100
     - NUM = OPTS.maxParallel / OPTS.repeat
     - in different microloops (for async parallelism)
     - in different processes (for CPU parallelism) (not as important)
     - maybe switch to ava to get that
     - should stop on failure if OPTS.stopOnFail true (def)
  - pass options to underlying test runner, e.g. reporters, reporting options, include|exclude, etc.
     - all those concerns should be offloaded to the test runner
     - maybe add OPTS.runner to choose between different runners (i.e. they would be adapters)
     - maybe force a specific runner???
     - maybe only allow specific runner options???
     - maybe force output to TAP format to allow different reporters???
  - probably:
     - use library to maintain NUM workers, where NUM is number of CPU cores
     - assign equal number of it() to each
        - they each spawn the same test file, but with a modulo to filter which tests they define
     - force a specific test runner:
        - should allow concurrent tests to have a maxParallel limit
        - should have lots of features, which should be directly accessible (and whitelisted) from user perspective
        - should report in TAP format, and add --reporter=MODULE option that points to tap reporter
  - --dry:
     - when --dry, should not show any reporter output
     - but should print the first thrown error.message

Custom properties:
  - config|tasks|task.x-*
  - are simply ignored

Content negotiation:
  - autoformat
     - use it in stringify/parse
     - add multipart/form-data format:
        - parsed as { "VAR[;filename=FILE][;content-type=MIME]": VAL, ... }
        - stringify not on plain object should throw
        - each VAL is parsed according to that MIME, or left as unparsed string otherwise
     - should maybe default to raw format instead of throwing exception
  - Content-Type [C]:
     - intersection of:
        - types supported by library (for serialization)
        - merge (override) of:
           - def: application/json
           - spec.consumes
           - test.request.headers.content-type
              - matched case-insensitive
              - if specified, must only use:
                 - type 'string'
                 - enum
                 - default|title|description
     - intersection:
        - if empty array, throw that MIME is unsupported
        - should work with MIMEs being just '+EXT'
        - MIME options:
           - if none specified, leave as is
           - if specified by one side or both sides and equal, pick it
           - if specified by both sides and different, do not pick that MIME
  - Accept [C]:
     - same but with:
        - types supported by library (for parsing)
        - def: any
        - spec.produces
        - test.request.headers.accept
  - Content-Type [S]
     - if not among possible library parsers, throw error
     - used to pick how to parse body
     - def: application/octet-stream
     - validate according to same logic as other standard headers
  - [SMALL_]SCHEMA.type 'file':
     - PARAM:
        - during spec normalization, set Content-Type [C] to multipart/form-data
        - adds parameter filename="PARAM_NAME"
     - RESP:
        - validate Content-Type [S] is multipart/form-data
        - validates every part has "filename"
     - when normalizing OpenAPI schema to JSON schema:
        - convert to another type by guessing from other properties, and defaulting to string
        - should propose a PR to OPENAPI-SCHEMA-TO-JSON-SCHEMA (which throws at the moment)

Produce code coverage report

Add fuzzy testing values???
  - maybe propose as an option to JSON-SCHEMA-FAKER???

Also test middleware (not endpoints)

Add support for $data in x-tests JSON schemas
  - should be able to target merged response body, response headers, request
    parameters

Explore:
  - test runners, reporting
  - fuzzy testing
  - stress testing
  - load testing
  - test coverage

To improve???
  - reduce amount of mocking needed by users
  - reduce amount of setup and teardown (e.g. starting server) needed by users
     - including faking authentication

Plugins:
  - move plugins to own repositories
  - create a repository with some core plugins already included, so users don't have to
  - remove CORE_PLUGINS array

Schemes:
  - support over protocols than HTTP
     - including WebSocket
  - support OPERATION.schemes
  - unless OPTS.endpoint was given, should randomly pick protocol among SPEC|OPERATION.schemes for each test

Think about using OpenAPI 2.0 collectionFormat or OpenAPI 3.0 styles outside of OpenAPI

Add support for HTTP authentication, OAuth2 and OpenID

Add support for OpenAPI 3.0, RAML, API blueprint
  - should use a specification abstraction layer
  - if api-elements is not good enough, create own

Re-use autoserver's ajv error beautification utility (separate it in a different module)

Load test file re-using same logic as autoserver to allow many formats ("autoformat")

Globbing:
  - go through globbing in to_learn.txt to improve current usage of globbing (e.g. in config.tasks or in 'glob' plugin)

Dynamic properties keys:
  - problematic on parameter generation and response validation
     - for parameter name, response header names, OpenAPI schema properties
     - OpenAPI only allows static keys
  - possible solution:
     - use 'REGEXP' for keys
     - transform to patternProperties instead of properties
        - both ajv and JSON-SCHEMA-FAKER should then handle it correctly
     - how to mark that a key is a REGEXP not a STR??? Possible solutions:
        - all keys are REGEXPs
        - "/.../" key
        - global OPT
        - testOpt
        - property in specification, e.g. x-*
     - need to work with other specification formats too

Add configuration file and logic:
  - re-use same logic as autoserver
  - including JSON references (including in task files)

Check autoserver for feature suggestions:
  - including special linting tool

JSON schema v7:
  - allow v7 in OpenAPI SCHEMA
  - problems:
     - not supported by OpenAPI yet, i.e. OPENAPI-SCHEMA-TO-JSON-SCHEMA outputs JSON schema v4, but this is not a problem
     - however does not work with JSON-SCHEMA-FAKER yet

Cancel other sub-tests once one sub-test failed:
  - including HTTP request using AbortController:
     - not supported by node-fetch yet: https://github.com/bitinn/node-fetch/pull/437
  - including writing request bodies and reading response bodies (abort STREAMs)

Allow [SMALL_]SCHEMA.x-validate:
  - custom validation similar to autoserver's
  - needs to be added to AJV
  - problem with JSON-SCHEMA-FAKER:
     - not supported as is
     - might be even harder when it comes to combining with not|allOf|anyOf|oneOf

autopackage:
  - abtraction of package.json for many programming languages and OS package managers
  - automatically creates other package.json-like files
  - should whitelist the format we want,  to avoid pollution???
  - automatically publish:
     - maybe using a common user controlled by me
  - paying option:
     - to allow publishing
     - GitHub hooks

Business idea:
  - CI features:
     - executes requests on cloud/FaaS
     - notifications (GitHub hooks, etc.)
  - configuration editor
     - linting
     - execute as you type
  - nice reporting
