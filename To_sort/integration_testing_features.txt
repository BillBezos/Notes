OpenAPI spec merging should be optional:
  - no spec merging if testName does not contain any dot, or if first part is not an operationId
  - try to separate spec|operation from rest as much as possible, and make it optional
  - allow HTTP[S] URL for --spec option
  - switch from Sway to swagger-parser

URIS:
  - add test.request.path:
      - can use same values as other test.request.*, i.e. is generated
      - validate after generation:
          - type "string"
      - validate after {PARAM} replacement:
          - starts with "/" (unless "")
          - valid PATH syntax
      - can contain "...{PARAM}...":
          - replaced after generation using test.request.url.PARAM
      - merged to spec /PATH
          - def: ""
      - check that URL params exist in test.request.path
 - add test.request.server:
     - can use same values as other test.request.*, i.e. is generated
     - validate after generation:
         - type "string"
         - required
     - validate after {PARAM} replacement:
         - valid ORIGIN syntax
     - can contain "...{PARAM}...":
          - replaced after generation using test.request.url.PARAM
     - merged to spec.host (and related)
          - def: http, ENV_VAR HOST and PORT
     - remove --server option (use each.server instead)
  - add test.method:
     - can use same values as other test.request.*, i.e. is generated
     - validate after generation:
         - type "string"
         - among possible methods
     - case insensitive
     - merged to spec method
         - def: GET
  - do not attach operation on task.* anymore. Instead do everything in beforeAll()

Final response:
  - for a given repeated task, if any plugin throws:
     - it should not run following plugins for that repeated task
     - but other repeated tasks should still run
         - fix a long comment saying otherwise
  - wrap with an error handler each repeated task so it either:
     - returns { task } on success
     - returns (not throw) { error } on exception
 - CONF.repeat plugins:
     - sets { tasks, errors } with all of them
     - sets { task, error } with first of them (undefined if none)
     - pass both to second set of plugins, i.e. they do not have to be repeat-aware but can be (depending on whether they use plural or singular properties)
     - but return only singular version to final return value (i.e. only one result per task).
  - bail behavior???
  - then run a second set of plugins, after the main one:
      - includes the reporters
      - i.e. they run even if one main plugin threw
      - they are not affected by CONF.repeat, i.e. only run once per task
      - they can throw themselves, giving same result as if task failed (i.e. returning { error })
      - they can change { task[s]|error[s] } (although we do not need it at the moment)
  - top-level command should return PROMISE:
      - resolving to ARR of  { type: 'task', ...TASK_OBJ }
      - or rejecting to ERROR with ERROR.errors OBJ_ARR
  - afterAll() cannot change tasks OBJ_ARR (because they might have alreadt been streamed) but it can return other properties that are shallowly merged
      - what if it throws???
  - CLI should:
      - set exit code 1 if PROMISE was rejected
      - print error and its tack trace if PROMISE was rejected with a "bug" error

Reporting:
  - do not use any test runner
  - should be two plugins:
      - first produces TAP
         - use own TAP producing code, combining strengths of supertap and tape
         - do it on beforeAll() (TAP version, unique test), afterEach() (assertion), afterAll() (plan, closing comments, closing the stream)
         - should build title there
      - second uses TAP consuming libraries to perform many different reporting
         - including printing TAP as is
         - pipe to CONF.reporter.format:
             - true (def): stdout
             - false: silent
             - "PATH": file
         - CONF.reporter.style "REPORTER"
            - hand picked set of best TAP reporters for each category
            - are separate plugins, i.e. "REPORTER" is just plugin name, looked up in node modules
                - reporters have different API than other plugins, and are called from the main reporting plugin
            - also CONF.reporter.options OBJ
            - create our own best TAP reporter specialized for our purposes
  - allow tests GLOB to be "-" to refer to stdin???
  - TAP output:
     - should probably include options (like repeat, maxParallel) as final comments (so they are printed but only once)
     - should use YAML props (includes expected|actual) for errors
     - reporters should use error.request|response
        - e.g. our default reporter should:
           - show "Request was:...
           - first show "Response was:..." if response body or header validation error (include emptiness check)

Fix opts.dry (should not run any of afterEach())

Load at dependencies between plugins???

Should add headers added by default by node-fetch to returnValue.request.*???

Return value: instead of hard-coding that only request|response should be returned, let plugins decide on it
  - however originalTask.* should not be overriden

Validation:
  - do not allow test.* keys that are not a PLUGIN_NAME
  - each plugin is responsible of its own validation
  - PLUGIN.validateConfig JSON_SCHEMA: validated against CONF.PLUGIN_NAME
  - PLUGIN.validateTest JSON_SCHEMA: validated against test.PLUGIN_NAME
  - PLUGIN.beforeAll() can be used for more sophisticated validation
  - CLI options should take into account that things are organized as plugins

Support collectionFormat `multi` on query parameter serialization

Plugin architecture:
  - plugin configuration:
     - CONF.PLUGIN_NAME: overall configuration
     - test.PLUGIN_NAME: request-specific configuration
  - plugin exports a PLUGIN object with callback functions:
      - PLUGIN.beforeAll({ conf, tests, runTests }): at very beginning
      - PLUGIN.beforeEach({ conf, test, runTest }): before each fetch
      - PLUGIN.afterEach({ conf, test }): after each fetch
         - do distinction between beforeEach() and afterEach()???
      - PLUGIN.afterAll({ conf, tests }): at very end
  - callback functions can modify (replacing them) their arguments (CONF, test.*) by returning them
  - callback functions:
      - can be async/awaited
      - but only follow promise if a promise is returned (like the promiseThen() utility from autoserver) to avoid too many microtasks
         - in the same idea, Promise.all(ARR) should become simply ARR if none is a promise
  - separate everything to plugins:
      - configuration loading and validation
      - tests loading
      - repeat and parallel
      - depReqs
      - spec merging
      - request generation
      - request serialization
      - actual fetch
      - response parsing
      - response validation
  - other possible plugins: load testing, saving to files, selecting tests, reporting, etc.
  - whether a plugin is enabled or not by default is plugin-specific
  - PLUGIN.order FLOAT:
      - exported by each plugin
      - in beforeAll|beforeEach, plugins are run in increasing order
      - in afterAll|afterEach, inverse
      - if several plugins have same order, they are run in parallel
  - PLUGIN.name "PLUGIN_NAME"
  - PLUGIN.required "PLUGIN_NAME"_ARR: throw error if other plugin is not installed
  - all plugins installed in node_modules are loaded by default
  - PLUGIN.overrides "PLUGIN_NAME"_ARR: disable other plugins, e.g. to override core plugins
  - validate exported PLUGINs with a JSON schema
  - any error thrown from a plugin should be error.type should be "PLUGIN_NAME"
      - wrap each plugin with error handler to make that automatic and ensure it
      - or maybe dissociate `type` and `plugin`???

Fix README.md

Fix api-service

Test running:
  - should run NUM it() in parallel
     - NUM = OPTS.maxParallel / OPTS.repeat
     - in different microloops (for async parallelism)
     - in different processes (for CPU parallelism) (not as important)
     - maybe switch to ava to get that
     - should stop on failure if OPTS.stopOnFail true (def)
  - pass options to underlying test runner, e.g. reporters, reporting options, include|exclude, etc.
     - all those concerns should be offloaded to the test runner
     - maybe add OPTS.runner to choose between different runners (i.e. they would be adapters)
     - maybe force a specific runner???
     - maybe only allow specific runner options???
     - maybe force output to TAP format to allow different reporters???
  - probably:
     - use library to maintain NUM workers, where NUM is number of CPU cores
     - assign equal number of it() to each
        - they each spawn the same test file, but with a modulo to filter which tests they define
     - force a specific test runner:
        - should allow concurrent tests to have a maxParallel limit
        - should have lots of features, which should be directly accessible (and whitelisted) from user perspective
        - should report in TAP format, and add --reporter=MODULE option that points to tap reporter
  - things I might need the test runner for:
     - reporters
     - test REGEX selection
     - parallelism
     - allow test option to be a directory searched recursively
     - exclude option
     - test.only() and test.skip()
     - bail
  - --dry:
     - when --dry, should not show any reporter output
     - but should print the first thrown error.message

Content negotiation:
  - Content-Type [C]:
     - intersection of:
        - types supported by library (for serialization)
        - merge (override) of:
           - def: application/json
           - spec.consumes
           - test.request.headers.content-type
              - matched case-insensitive
              - if specified, must only use:
                 - type 'string'
                 - enum
                 - default|title|description
     - intersection:
        - if empty array, throw that MIME is unsupported
        - should work with MIMEs being just '+EXT'
        - MIME options:
           - if none specified, leave as is
           - if specified by one side or both sides and equal, pick it
           - if specified by both sides and different, do not pick that MIME
  - Accept [C]:
     - same but with:
        - types supported by library (for parsing)
        - spec.produces
        - test.request.headers.accept
  - Content-Type [S]
     - if not among possible library parsers, throw error
     - used to pick how to parse body
     - def: application/octet-stream
     - validate according to same logic as other standard headers
  - add response body parsing for:
     - x-www-url-encoded (using QUERYSTRING) as an OBJ
        - use body-parser library instead???
     - multipart/form-data (using another library) as an OBJ
  - add request param serializating for:
     - x-www-form-urlencoded and multipart/form-data:
        - PARAM must be an OBJ:
           - for body: [SMALL_]SCHEMA.type must be 'object'
           - for formData: it is always an OBJ
        - do not set multipart/form-data parameter Content-Type
  - add support for formData PARAM:
     - validate Content-Type [C] can only be x-www-form-urlencoded or multipart/form-data
     - uses { PARAM_NAME: generatedValue, ... }
     - there can be several PARAMs of that type
  - [SMALL_]SCHEMA.type 'file':
     - PARAM:
        - validate Content-Type [C] is multipart/form-data
        - adds parameter filename="PARAM_NAME"
     - RESP:
        - validate Content-Type [S] is multipart/form-data
        - validates every part has "filename"
     - when normalizing OpenAPI schema to JSON schema:
        - convert to another type by guessing from other properties, and defaulting to string
        - should propose a PR to OPENAPI-SCHEMA-TO-JSON-SCHEMA (which throws at the moment)

Produce code coverage report

Add fuzzy testing values???
  - maybe propose as an option to JSON-SCHEMA-FAKER???

Also test middleware (not endpoints)

Add support for $data in x-tests JSON schemas
  - should be able to target merged response body, response headers, request
    parameters

Explore:
  - test runners, reporting
  - fuzzy testing
  - stress testing
  - load testing
  - test coverage

To improve???
  - reduce amount of mocking needed by users
  - reduce amount of setup and teardown (e.g. starting server) needed by users
     - including faking authentication

Divide repository into smaller modules: request building, response validation, etc.

Schemes:
  - support over protocols than HTTP
     - including WebSocket
  - support OPERATION.schemes
  - unless OPTS.endpoint was given, should randomly pick protocol among SPEC|OPERATION.schemes for each test

Add support for HTTP authentication, OAuth2 and OpenID

Add support for OpenAPI 3.0, RAML, API blueprint
  - should use a specification abstraction layer
  - if api-elements is not good enough, create own

Re-use autoserver's ajv error beautification utility (separate it in a different module)

Load test file re-using same logic as autoserver to allow many formats ("autoformat")
  - should validate against circular references, but by adding this as a feature to autoformat

Dynamic properties keys:
  - problematic on parameter generation and response validation
     - for parameter name, response header names, OpenAPI schema properties
     - OpenAPI only allows static keys
  - possible solution:
     - use 'REGEXP' for keys
     - transform to patternProperties instead of properties
        - both ajv and JSON-SCHEMA-FAKER should then handle it correctly
     - how to mark that a key is a REGEXP not a STR??? Possible solutions:
        - all keys are REGEXPs
        - "/.../" key
        - global OPT
        - testOpt
        - property in specification, e.g. x-*
     - need to work with other specification formats too

Add configuration file and logic:
  - re-use same logic as autoserver

Check autoserver for feature suggestions:
  - including special linting tool

JSON schema v7:
  - allow v7 in OpenAPI SCHEMA
  - problems:
     - not supported by OpenAPI yet, i.e. OPENAPI-SCHEMA-TO-JSON-SCHEMA outputs JSON schema v4, but this is not a problem
     - however does not work with JSON-SCHEMA-FAKER yet

Cancel other sub-tests once one sub-test failed:
  - including HTTP request using AbortController:
     - not supported by node-fetch yet: https://github.com/bitinn/node-fetch/pull/437
  - including writing request bodies and reading response bodies (abort STREAMs)

Allow [SMALL_]SCHEMA.x-validate:
  - custom validation similar to autoserver's
  - needs to be added to AJV
  - problem with JSON-SCHEMA-FAKER:
     - not supported as is
     - might be even harder when it comes to combining with not|allOf|anyOf|oneOf

autopackage:
  - abtraction of package.json for many programming languages and OS package managers
  - automatically creates other package.json-like files
  - should whitelist the format we want,  to avoid pollution???
  - automatically publish:
     - maybe using a common user controlled by me
  - paying option:
     - to allow publishing
     - GitHub hooks

Business idea:
  - CI features:
     - executes requests on cloud/FaaS
     - notifications (GitHub hooks, etc.)
  - configuration editor
     - linting
     - execute as you type
  - nice reporting
